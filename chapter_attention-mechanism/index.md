# Attention Mechanisms
:label:`chap_attention`

As a bit of a historical digression, attention research is an enormous field with a long history in cognitive neuroscience. Focalization, concentration of consciousness are of the essence of attention, which enable the human to prioritize the perception in order to deal effectively with others. As a result, we do not process all the information that is available in the sensory input. At any time, we are aware of only a small fraction of the information in the environment. In cognitive neuroscience, there are several types of attention such as selective attention, covert attention, and spatial attention. The theory ignites the spark in recent deep learning is the *feature integration theory* of the selective attention, which was developed by Anne Treisman and Garry Gelade through the paper :cite:`Treisman.Gelade.1980` in 1980. This paper declares that when perceiving a stimulus, features are registered early, automatically, and in parallel, while objects are identified separately and at a later stage in processing. The theory has been one of the most influential psychological models of human visual attention. 

However, we will not indulge in too much theory of attention in neuroscience, but rather focus on applying the attention idea in deep learning,
where attention can be seen as a generalized pooling method with bias alignment over inputs.
In this chapter, we will provide you with some intuition about how to transform the attention idea to the concrete mathematics models, and make them work.

```toc
:maxdepth: 2

attention
seq2seq-attention
transformer
```

