# Attention Mechanism
:label:`chap_attention`

Attention is a generalized pooling method with bias alignment over inputs.

```toc
:maxdepth: 2

attention
seq2seq-attention
transformer
```

