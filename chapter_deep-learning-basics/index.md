# Deep Learning Basics 

This chapter serves as a basic introduction to deep learning.  Deep
learning, as part of a broader family of machine learning methods, is
often based on a neural network model in order to represent
increasingly abstract concepts or patterns on a step-by-step basis. In
order to briefly introduce basic machine learning concepts, we will
begin with two single-layer neural networks, linear regression and
softmax regression,  then continue from single-layer neural networks
to multi-layer neural networks, and finally, introduce a deep learning
model through a multi-layer perceptron. After observing and
understanding the model overfitting, we will introduce two
frequently-adopted methods of dealing with it in deep learning: weight
decay and dropout. Next, to further understand deep learning model
training, we will explain forward propagation and back propagation in
depth.  After a reader is familiar with these two concepts, they will
better understand some of the problems with numerical stability and
initialization in deep learning. Finally, we will apply the knowledge
learned in this chapter to a deep learning application case. 

```eval_rst

.. toctree::
   :maxdepth: 2

   linear-regression
   linear-regression-scratch
   linear-regression-gluon
   softmax-regression
   fashion-mnist
   softmax-regression-scratch
   softmax-regression-gluon
   mlp
   mlp-scratch
   mlp-gluon
   underfit-overfit
   weight-decay
   dropout
   backprop
   numerical-stability-and-init
   kaggle-house-price

```
