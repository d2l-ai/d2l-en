# Linear Regression

To get our feet wet, we'll start off by looking at the problem of regression.
This is the task of predicting a *real valued target* $y$ given a data point $x$.
Regression problems are extremely common in practice. For example, they are used for predicting continuous values, such as house prices, temperatures, sales, and so on. This is quite different from classification problems (which we study later), where the outputs are discrete (such as apple, banana, orange, etc. in image classification). 

## Basic Elements of Linear Regression

In linear regression, the simplest and still perhaps the most useful approach,
we assume that prediction can be expressed as a *linear* combination of the input features 
(thus giving the name *linear* regression). 

### Linear Model

For the sake of simplicity we we will use the problem of estimating the price of a house based (e.g. in dollars) on area (e.g. in square feet) and age (e.g. in years) as our running example. In this case we could model 

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b$$

While this is quite illustrative, it becomes extremely tedious when dealing with more than two variables (even just naming them becomes a pain). This is what mathematicians have invented vectors for. In the case of $d$ variables we get 

$$\hat{y} = w_1 \cdot x_1 + ... + w_d \cdot x_d + b$$

Given a collection of data points $X$, and corresponding target values $\boldsymbol{y}$, 
we'll try to find the *weight* vector $w$ and bias term $b$ 
(also called an *offset* or *intercept*)
that approximately associate data points $x_i$ with their corresponding labels $y_i$.
Using slightly more advanced math notation, we can express the long sum as $\hat{y} = w^\top x + b$. Finally, for a collection of data points $X$ the predictions $\hat{y}$ can be expressed via the matrix-vector product:

$${\hat{y}} = X \boldsymbol{w} + b$$

It's quite reasonable to assume that the relationship between $x$ and $y$ is only approximately linear. There might be some error in measuring things. Likewise, while the price of a house typically decreases, this is probably less the case with very old historical mansions which are likely to be prized specifically for their age. To find the parameters $w$ we need two more things: some way to measure the quality of the current model and secondly, some way to manipulate the model to improve its quality. 

### Training Data

The first thing that we need is data, such as the actual selling price of multiple houses as well as their corresponding area and age. We hope to find model parameters on this data to minimize the error between the predicted price and the real price of the model. In the terminology of machine learning, the data set is called a ‘training data’ or ‘training set’, a house (often a house and its price) is called a ‘sample’, and its actual selling price is called a ‘label’. The two factors used to predict the label are called ‘features’ or 'covariates'. Features are used to describe the characteristics of the sample. 

Typically we denote by $n$ the number of samples that we collect. Each sample (indexed as $i$) is described by $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$, and the label is $y^{(i)}$. 

### Loss Function

In model training, we need to measure the error between the predicted value and the real value of the price. Usually, we will choose a non-negative number as the error. The smaller the value, the smaller the error. A common choice is the square function. The expression for evaluating the error of a sample with an index of $i$ is as follows:

$$l^{(i)}(w, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,$$

The constant $1/2$ ensures that the constant coefficient, after deriving the quadratic term, is 1, which is slightly simpler in form. Obviously, the smaller the error, the closer the predicted price is to the actual price, and when the two are equal, the error will be zero. Given the training data set, this error is only related to the model parameters, so we record it as a function with the model parameters as parameters. In machine learning, we call the function that measures the error the ‘loss function’. The squared error function used here is also referred to as ‘square loss’.

To make things a bit more concrete, consider the example below where we plot such a regression problem for a one-dimensional case, e.g. for a model where house prices depend only on area. 

![Linear regression is a single-layer neural network. ](../img/linearregression.svg)

As you can see, large differences between estimates $\hat{y}^{(i)}$ and observations $y^{(i)}$ lead to even larger contributions in terms of the loss, due to the quadratic dependence. To measure the quality of a model on the entire dataset, we can simply average the losses on the training set. 

$$L(w, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(w, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(w^\top x^{(i)} + b - y^{(i)}\right)^2.$$

In model training, we want to find a set of model parameters, represented by $w^*, b^*$, that can minimize the average loss of training samples:

$$w^*, b^* = \operatorname*{argmin}_{w, b}\  L(w, b).$$


### Optimization Algorithm

When the model and loss function are in a relatively simple format, the solution to the aforementioned loss minimization problem can be expressed analytically in a closed form solution, involving matrix inversion. This is very elegant, it allows for a lot of nice mathematical analysis, *but* it is also very restrictive insofar as this approach only works for a small number of cases (e.g. multilayer perceptrons and nonlinear layers are no go). Most deep learning models do not possess such analytical solutions. The value of the loss function can only be reduced by a finite update of model parameters via an incremental optimization algorithm. 

The mini-batch stochastic gradient descent is widely used for deep learning to find numerical solutions. Its algorithm is simple: first, we initialize the values of the model parameters, typically at random; then we iterate over the data multiple times, so that each iteration may reduce the value of the loss function. In each iteration, we first randomly and uniformly sample a mini-batch $\mathcal{B}$ consisting of a fixed number of training data examples; we then compute the derivative (gradient) of the average loss on the mini batch the with regard to the model parameters. Finally, the product of this result and a predetermined step size $\eta > 0$ is used to change the parameters in the direction of the minimum of the loss. In math we have

$$(w,b) \leftarrow (w,b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(w,b)} l^{(i)}(w,b)$$

For quadratic losses and linear functions we can write this out explicitly as followsn Note that $w$ and $x$ are vectors. Here the more elegant vector notation makes the math much more readable than expressing things in terms of coefficients, say $w_1, w_2, \ldots w_d$. 

$$
\begin{align}
w &\leftarrow w -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_w l^{(i)}(w, b) && =
w - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} x^{(i)} \left(w^\top x^{(i)} + b - y^{(i)}\right),\\
b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(w, b)  && =
b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(w^\top x^{(i)} - y^{(i)}\right).
\end{align}
$$

In the above equation $|\mathcal{B}|$ represents the number of samples (batch size) in each mini-batch, $\eta$ is referred to as ‘learning rate’ and takes a positive number. It should be emphasized that the values of the batch size and learning rate are set somewhat manually and are typically not learned through model training. Therefore, they are referred to as *hyper-parameters*. What we usually call *tuning hyper-parameters* refers to the adjustment of these terms. In the worst case this is performed through repeated trial and error until the appropriate hyper-parameters are found. A better approach is to learn these as parts of model training. This is an advanced topic and we do not cover them here for the sake of simplicity. 

### Model Prediction

After model training has been completed, we then record the values of the model parameters $w, b$ as $\hat{w}, \hat{b}$. Note that we do not necessarily obtain the optimal solution of the loss function minimizer, $w^*, b^*$ (or the true parameters), but instead we gain an approximation of the optimal solution. We can then use the learned linear regression model $\hat{w}^\top x + \hat{b}$ to estimate the price of any house outside the training data set with area (square feet) as $x_1$ and house age (year) as $x_2$. Here, estimation also referred to as ‘model prediction’ or ‘model inference’. 

Note that calling this step 'inference' is actually quite a misnomer, albeit one that has become the default in deep learning. In statistics 'inference' means estimating parameters and outcomes based on other data. This misuse of terminology in deep learning can be a source of confusion when talking to statisticians. We adopt the incorrect, but by now common, terminology of using 'inference' when a (trained) model is applied to new data (and express our sincere apologies to centuries of statisticians). 


## From Linear Regression to Deep Networks

So far we only talked about linear functions. Neural Networks cover a lot more than that. That said, linear functions are an important building block. Let's start by rewriting things in a 'layer' notation. 

### Neural Network Diagram

While in deep learning, we can represent model structures visually using neural network diagrams. To more clearly demonstrate the linear regression as the structure of neural network, Figure 3.1 uses a neural network diagram to represent the linear regression model presented in this section. The neural network diagram hides the weight and bias of the model parameter.

![Linear regression is a single-layer neural network. ](../img/singleneuron.svg)

In the neural network shown above, the inputs are $x_1, x_2, \ldots x_d$. Sometimes the number of inputs is also referred as feature dimension. In the above cases the number of inputs is $d$ and the number of outputs is $1$. It should be noted that we use the output directly as the output of linear regression.  Since the input layer does not involve any other nonlinearities or any further calculations, the number of layers is 1. Sometimes this setting is also referred to as a single neuron. Since all inputs are connected to all outputs (in this case it's just one), the layer is also referred to as a 'fully connected layer' or 'dense layer'. 

### A Detour to Biology

Neural networks quite clearly derive their name from Neuroscience. To understand a bit better how many network architectures were invented, it is worth while considering the basic structure of a neuron. For the purpose of the analogy it is sufficient to consider the *dendrites* (input terminals), the *nucleus* (CPU), the *axon* (output wire), and the *axon terminals* (output terminals) which connect to other neurons via *synapses*.  

![The real neuron](../img/Neuron.svg)

Information $x_i$ arriving from other neurons (or environmental sensors such as the retina) is received in the dendrites. In particular, that information is weighted by *synaptic weights* $w_i$ which determine how to respond to the inputs (e.g. activation or inhibition via $x_i w_i$). All this is aggregated in the nucleus $y = \sum_i x_i w_i + b$, and this information is then sent for further processing in the axon $y$, typically after some nonlinear processing via $\sigma(y)$. From there it either reaches its destination (e.g. a muscle) or is fed into another neuron via its dendrites. 

Brain *structures* can be quite varied. Some look rather arbitrary whereas others have a very regular structure. E.g. the visual system of many insects is quite regular. The analysis of such structures has often inspired neuroscientists to propose new architectures, and in some cases, this has been successful. Note, though, that it would be a fallacy to require a direct correspondence - just like airplanes are *inspired* by birds, they have many distinctions. An equal source of inspiration have been mathematics and compute science. 

### Vectorzation for Speed

In model training or prediction, we often use vector calculations and process multiple observations at the same time. To illustrate why this matters, consideer two methods of adding vectors. We begin by creating two 1000 dimensional ones first.

```{.python .input  n=1}
from mxnet import nd
from time import time

a = nd.ones(shape=10000)
b = nd.ones(shape=10000)
```

One way to add vectors is to add them one coordinate at a time using a for loop.

```{.python .input  n=2}
start = time()
c = nd.zeros(shape=10000)
for i in range(10000):
    c[i] = a[i] + b[i]
time() - start
```

Another way to add vectors is to add the vectors directly:

```{.python .input  n=3}
start = time()
d = a + b
time() - start
```

Obviously, the latter is vastly faster than the former. Vectorizing code is a good way of getting order of mangitude speedups. Likewise, as we saw above, it also greatly simplifies the mathematics and with it, it reduces the potential for errors in the notation. 

## The Normal Distribution and Squared Loss

The following is optional and can be skipped but it will greatly help with understanding some of the design choices in building deep learning models. As we saw above, using the squred loss $l(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2$ has many nice properties, such as having a particularly simple derivative $\partial_{\hat{y}} l(y, \hat{y}) = (\hat{y} - y)$. That is, the gradient is given by the difference between estimate and observation. You might reasonably point out that linear regression is a [classical](https://en.wikipedia.org/wiki/Regression_analysis#History) statistical model. Legendre first developed the method of least squares regression in 1805, which was shortly thereafter rediscovered by Gauss in 1809. To understand this a bit better, recall the normal distribution with mean $\mu$ and variance $\sigma^2$. 

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$$

It can be visualized as follows:

```{.python .input  n=2}
%matplotlib inline
from matplotlib import pyplot as plt
from IPython import display
from mxnet import nd
import math

x = nd.arange(-7, 7, 0.01)
# mean and variance pairs
parameters = [(0,1), (0,2), (3,1)]

# display SVG rather than JPG 
display.set_matplotlib_formats('svg')
plt.figure(figsize=(10, 6))
for (mu, sigma) in parameters:
    p = (1/math.sqrt(2 * math.pi * sigma**2)) * nd.exp(-(0.5/sigma**2) * (x-mu)**2)
    plt.plot(x.asnumpy(), p.asnumpy(), label='mean ' + str(mu) + ', variance ' + str(sigma))

plt.legend()
plt.show()
```

```{.json .output n=2}
[
 {
  "data": {
   "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (http://matplotlib.org/) -->\n<svg height=\"360.738125pt\" version=\"1.1\" viewBox=\"0 0 605.165625 360.738125\" width=\"605.165625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 360.738125 \nL 605.165625 360.738125 \nL 605.165625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 336.86 \nL 594.465625 336.86 \nL 594.465625 10.7 \nL 36.465625 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me444716ea0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.088928\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- \u22126 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-2212\"/>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-36\"/>\n      </defs>\n      <g transform=\"translate(90.717834 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.608261\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- \u22124 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-34\"/>\n      </defs>\n      <g transform=\"translate(163.237167 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.127594\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- \u22122 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-32\"/>\n      </defs>\n      <g transform=\"translate(235.756501 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.646927\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-30\"/>\n      </defs>\n      <g transform=\"translate(312.465677 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"388.166261\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2 -->\n      <g transform=\"translate(384.985011 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"460.685594\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 4 -->\n      <g transform=\"translate(457.504344 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"533.204927\" xlink:href=\"#me444716ea0\" y=\"336.86\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(530.023677 351.458437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9184b48688\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"322.034545\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-2e\"/>\n      </defs>\n      <g transform=\"translate(7.2 325.833764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"284.872643\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.05 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-35\"/>\n      </defs>\n      <g transform=\"translate(7.2 288.671862)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"247.71074\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-31\"/>\n      </defs>\n      <g transform=\"translate(7.2 251.509959)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"210.548838\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.15 -->\n      <g transform=\"translate(7.2 214.348057)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"173.386935\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.20 -->\n      <g transform=\"translate(7.2 177.186154)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"136.225033\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 140.024252)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"99.063131\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-33\"/>\n      </defs>\n      <g transform=\"translate(7.2 102.862349)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"61.901228\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.35 -->\n      <g transform=\"translate(7.2 65.700447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9184b48688\" y=\"24.739326\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.40 -->\n      <g transform=\"translate(7.2 28.538544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p4bf5fef154)\" d=\"M 61.829261 322.034545 \nL 171.333454 321.926815 \nL 182.573948 321.681928 \nL 189.825883 321.314451 \nL 195.264828 320.836277 \nL 199.978589 320.205073 \nL 203.967149 319.451737 \nL 207.59312 318.537641 \nL 210.856496 317.480444 \nL 213.757266 316.314206 \nL 216.658037 314.895152 \nL 219.196203 313.41289 \nL 221.734385 311.673766 \nL 224.272568 309.644691 \nL 226.810734 307.290679 \nL 228.986329 304.986627 \nL 231.161907 302.393374 \nL 233.337485 299.48696 \nL 235.513063 296.243486 \nL 237.688641 292.639454 \nL 239.864219 288.65219 \nL 242.402402 283.487561 \nL 244.940567 277.741518 \nL 247.47875 271.387663 \nL 250.016933 264.405421 \nL 252.555099 256.781188 \nL 255.093282 248.509377 \nL 257.994052 238.26851 \nL 260.894823 227.210063 \nL 264.158198 213.84575 \nL 267.784161 197.960609 \nL 271.77273 179.436779 \nL 277.211683 152.970444 \nL 287.36438 103.295896 \nL 290.990343 86.730677 \nL 293.891114 74.369392 \nL 296.429297 64.377937 \nL 298.967479 55.293622 \nL 301.143057 48.322173 \nL 302.95603 43.14168 \nL 304.769021 38.572621 \nL 306.582011 34.648089 \nL 308.032396 31.991923 \nL 309.482781 29.779205 \nL 310.933167 28.020389 \nL 312.020947 27.004314 \nL 313.108745 26.251007 \nL 314.196542 25.762551 \nL 315.284323 25.540273 \nL 316.37212 25.584751 \nL 317.4599 25.895851 \nL 318.547698 26.472753 \nL 319.635478 27.313908 \nL 320.723276 28.417033 \nL 321.811074 29.779205 \nL 323.261459 31.991923 \nL 324.711844 34.648089 \nL 326.162229 37.735165 \nL 327.975202 42.177768 \nL 329.788193 47.23885 \nL 331.963771 54.077155 \nL 334.139348 61.684249 \nL 336.677531 71.42957 \nL 339.578302 83.556329 \nL 342.841677 98.217736 \nL 346.830228 117.184527 \nL 354.807365 156.549986 \nL 360.608906 184.582086 \nL 364.597457 202.831453 \nL 368.223437 218.40442 \nL 371.486813 231.450512 \nL 374.387583 242.206249 \nL 377.288354 252.133668 \nL 380.189124 261.217002 \nL 382.72729 268.473137 \nL 385.26549 275.094187 \nL 387.803656 281.097788 \nL 390.341821 286.508046 \nL 392.880021 291.354035 \nL 395.418187 295.668469 \nL 397.956353 299.486938 \nL 400.494553 302.846692 \nL 403.032718 305.785597 \nL 405.570884 308.341624 \nL 408.109084 310.552008 \nL 410.64725 312.452624 \nL 413.185415 314.077729 \nL 415.723616 315.459528 \nL 418.624386 316.779032 \nL 421.525157 317.860534 \nL 424.788532 318.838139 \nL 428.414478 319.680751 \nL 432.403029 320.372781 \nL 437.11679 320.950359 \nL 442.555761 321.385934 \nL 449.445082 321.706948 \nL 459.235209 321.917912 \nL 475.914657 322.017572 \nL 539.006468 322.034544 \nL 569.101989 322.034545 \nL 569.101989 322.034545 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p4bf5fef154)\" d=\"M 61.829261 321.71024 \nL 77.783517 321.350868 \nL 90.111792 320.85765 \nL 100.264506 320.233296 \nL 108.966835 319.480494 \nL 116.581366 318.608459 \nL 123.470688 317.60773 \nL 129.997439 316.438105 \nL 136.161585 315.102753 \nL 141.963126 313.611948 \nL 147.402062 311.982961 \nL 152.478428 310.239485 \nL 157.554776 308.261265 \nL 162.268537 306.198034 \nL 166.982281 303.90251 \nL 171.69605 301.361759 \nL 176.409802 298.564366 \nL 181.123563 295.500778 \nL 185.837315 292.163772 \nL 190.551076 288.548854 \nL 195.264828 284.654742 \nL 199.978589 280.48369 \nL 205.054937 275.689327 \nL 210.131286 270.594447 \nL 215.570239 264.824442 \nL 221.371798 258.350708 \nL 227.898532 250.735662 \nL 235.875651 241.076544 \nL 260.16963 211.390509 \nL 265.971171 204.783408 \nL 271.047537 199.32524 \nL 275.761298 194.590076 \nL 280.112454 190.551128 \nL 284.101004 187.163628 \nL 287.726985 184.370231 \nL 291.352948 181.869836 \nL 294.616324 179.884855 \nL 297.879682 178.163385 \nL 301.143057 176.715635 \nL 304.043828 175.665571 \nL 306.944599 174.843598 \nL 309.845369 174.253649 \nL 312.746157 173.898548 \nL 315.646927 173.78 \nL 318.547698 173.898548 \nL 321.448469 174.253649 \nL 324.349239 174.843598 \nL 327.25001 175.66556 \nL 330.15078 176.715635 \nL 333.051568 177.98883 \nL 336.314926 179.680353 \nL 339.578302 181.636617 \nL 342.841677 183.846112 \nL 346.467641 186.582428 \nL 350.093604 189.596066 \nL 354.082155 193.206208 \nL 358.433311 197.462916 \nL 363.147071 202.40278 \nL 368.223437 208.043688 \nL 374.387583 215.242786 \nL 382.36472 224.93559 \nL 404.483104 252.025929 \nL 411.372425 259.997538 \nL 417.536571 266.781774 \nL 422.975542 272.447452 \nL 428.051873 277.436897 \nL 433.128239 282.120397 \nL 438.204605 286.486662 \nL 442.918366 290.251978 \nL 447.632092 293.73824 \nL 452.345853 296.948328 \nL 457.059613 299.888009 \nL 461.773374 302.565644 \nL 466.487135 304.991694 \nL 471.200896 307.178332 \nL 476.277227 309.280879 \nL 481.353593 311.139386 \nL 486.792529 312.881089 \nL 492.2315 314.387507 \nL 498.033041 315.761145 \nL 504.197187 316.986854 \nL 510.723938 318.056142 \nL 517.975864 319.009637 \nL 525.952966 319.822469 \nL 534.655312 320.483807 \nL 544.808009 321.028395 \nL 557.136301 321.455005 \nL 569.101989 321.704519 \nL 569.101989 321.704519 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p4bf5fef154)\" d=\"M 61.829261 322.034545 \nL 280.112454 321.926815 \nL 291.352948 321.681928 \nL 298.604875 321.314451 \nL 304.043828 320.836277 \nL 308.757589 320.205073 \nL 312.746157 319.451735 \nL 316.37212 318.537641 \nL 319.635478 317.480451 \nL 322.536266 316.314206 \nL 325.437037 314.895152 \nL 327.975202 313.41289 \nL 330.513385 311.673766 \nL 333.051568 309.644691 \nL 335.589734 307.290679 \nL 337.765312 304.986644 \nL 339.94089 302.39339 \nL 342.116485 299.48696 \nL 344.292063 296.243486 \nL 346.467641 292.639454 \nL 348.643219 288.65219 \nL 351.181401 283.487561 \nL 353.719584 277.741473 \nL 356.25775 271.387663 \nL 358.795916 264.405482 \nL 361.334116 256.781133 \nL 363.872281 248.509377 \nL 366.773052 238.26851 \nL 369.673822 227.210063 \nL 372.937198 213.84575 \nL 376.563144 197.960686 \nL 380.551729 179.436779 \nL 385.990665 152.970533 \nL 396.143397 103.295807 \nL 399.769343 86.730677 \nL 402.670113 74.369392 \nL 405.208279 64.377981 \nL 407.746479 55.293622 \nL 409.92204 48.322217 \nL 411.73503 43.14168 \nL 413.54802 38.572621 \nL 415.361011 34.648089 \nL 416.811396 31.991923 \nL 418.261781 29.779205 \nL 419.712166 28.020389 \nL 420.799947 27.004314 \nL 421.887727 26.251029 \nL 422.975542 25.762551 \nL 424.063322 25.540273 \nL 425.151103 25.584751 \nL 426.238917 25.895873 \nL 427.326698 26.472753 \nL 428.414478 27.313908 \nL 429.502258 28.417011 \nL 430.590073 29.779205 \nL 432.040459 31.991923 \nL 433.490844 34.648089 \nL 434.941229 37.735165 \nL 436.754219 42.177813 \nL 438.567175 47.238784 \nL 440.74277 54.077155 \nL 442.918366 61.684315 \nL 445.456531 71.42957 \nL 448.357302 83.556329 \nL 451.620677 98.217736 \nL 455.609228 117.184527 \nL 463.586364 156.549986 \nL 469.387906 184.582086 \nL 473.376456 202.831453 \nL 477.002437 218.40442 \nL 480.265812 231.450512 \nL 483.166583 242.206249 \nL 486.067354 252.133668 \nL 488.968124 261.217002 \nL 491.50629 268.473137 \nL 494.04449 275.094187 \nL 496.582655 281.097788 \nL 499.120821 286.508046 \nL 501.659021 291.354035 \nL 504.197187 295.668469 \nL 506.735352 299.486938 \nL 509.273553 302.846692 \nL 511.811718 305.785597 \nL 514.349884 308.341624 \nL 516.888084 310.552008 \nL 519.42625 312.452624 \nL 521.964415 314.077729 \nL 524.502581 315.459512 \nL 527.403386 316.779032 \nL 530.304156 317.860534 \nL 533.567497 318.83813 \nL 537.193478 319.680751 \nL 541.182029 320.372781 \nL 545.89579 320.950359 \nL 551.33476 321.385934 \nL 558.224082 321.706948 \nL 568.014208 321.917912 \nL 569.101989 321.931023 \nL 569.101989 321.931023 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 336.86 \nL 36.465625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 336.86 \nL 594.465625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 336.86 \nL 594.465625 336.86 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 10.7 \nL 594.465625 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 458.615625 62.734375 \nL 587.465625 62.734375 \nQ 589.465625 62.734375 589.465625 60.734375 \nL 589.465625 17.7 \nQ 589.465625 15.7 587.465625 15.7 \nL 458.615625 15.7 \nQ 456.615625 15.7 456.615625 17.7 \nL 456.615625 60.734375 \nQ 456.615625 62.734375 458.615625 62.734375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 460.615625 23.798437 \nL 480.615625 23.798437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- mean 0, variance 1 -->\n     <defs>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-6d\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-6e\"/>\n      <path id=\"DejaVuSans-20\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-2c\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-72\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-63\"/>\n     </defs>\n     <g transform=\"translate(488.615625 27.298437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-30\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-2c\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 460.615625 38.476562 \nL 480.615625 38.476562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_18\">\n     <!-- mean 0, variance 2 -->\n     <g transform=\"translate(488.615625 41.976562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-30\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-2c\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 460.615625 53.154687 \nL 480.615625 53.154687 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_19\">\n     <!-- mean 3, variance 1 -->\n     <g transform=\"translate(488.615625 56.654687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-2c\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4bf5fef154\">\n   <rect height=\"326.16\" width=\"558\" x=\"36.465625\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n",
   "text/plain": "<Figure size 720x432 with 1 Axes>"
  },
  "metadata": {
   "needs_background": "light"
  },
  "output_type": "display_data"
 }
]
```

As can be seen in the figure above, changing the mean shifts the function, increasing the variance makes it more spread-out with a lower peak. The key assumption in linear regression with least mean squares loss is that the observations actually arise from noisy observations, where noise is added to the data, e.g. as part of the observations process. 

$$y = w^\top x + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2)$$

This allows us to write out the *likelihood* of seeing a particular $y$ for a given $x$ via

$$p(y|x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - w^\top x - b)^2\right)$$

A good way of finding the most likely values of $b$ and $w$ is to maximize the *likelihood* of the entire dataset 

$$p(Y|X) = \prod_{i=1}^{n} p(y^{(i)}|x^{(i)})$$

The notion of maximizing the likelihood of the data subject to the parameters is well known as the *Maximum Likelihood Principle* and its estimators are usually called *Maximum Likelihood Estimators* (MLE). Unfortunately, maximizing the product of many exponential functions is pretty awkward, both in terms of implementation and in terms of writing it out on paper. Instead, a much better way is to minimize the *Negative Log-Likelihood* $-\log P(Y|X)$. In the above case this works out to be 

$$-\log P(Y|X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - w^\top x^{(i)} - b\right)^2$$

A closer inspection reveals that for the purpose of minimizing $-\log P(Y|X)$ we can skip the first term since it doesn't depend on $w, b$ or even the data. The second term is identical to the objective we initially introduced, but for the multiplicative constant $\frac{1}{\sigma^2}$. Again, this can be skipped if we just want to get the most likely solution. It follows that maximum likelihood in a linear model with additive Gaussian noise is  



## Summary

* Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself. 
* Vectorizing makes everything better (mostly math) and faster (mostly code).
* Minimizing an objective function and performing maximum likelihood can mean the same thing. 
* Linear models are neural networks, too.

## Exercises

1. Assume that we have some data $x_1, \ldots x_n \in \mathbb{R}$. Our goal is to find a constant $b$ such that $\sum_i (x_i - b)^2$ is minimized. 
    * Find the optimal closed form solution.
    * What does this mean in terms of the Normal distribution?
1. Assume that we want to solve the optimization problem for linear regression with quadratic loss explicitly in closed form. To keep things simple, you can omit the bias $b$ from the problem. 
    * Rewrite the problem in matrix and vector notation (hint - treat all the data as a single matrix).
    * Compute the gradient of the optimization problem with respect to $w$.
    * Find the closed form solution by solving a matrix equation. 
    * When might this be better than using stochastic gradient descent (i.e. the incremental optimization approach that we discussed above)? When will this break (hint - what happens for high-dimensional $x$, what if many observations are very similar)?.
1. Assume that the noise model governing the additive noise $\epsilon$ is the exponential distribution. That is, $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$.
    * Write out the negative log-likelihood of the data under the model $-\log p(Y|X)$. 
    * Can you find a closed form solution?
    * Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?
1. Compare the runtime of the two methods of adding two vectors using other packages (such as NumPy) or other programming languages (such as MATLAB).


## Scan the QR code to get to the [forum](https://discuss.gluon.ai/t/topic/6321)

![](../img/qr_linear-regression.svg)
