```{.python .input  n=1}
%load_ext d2lbook.tab
tab.interact_select(['mxnet', 'pytorch', 'tensorflow'])
```

# Softmax Regression Implementation from Scratch
:label:`sec_softmax_scratch`

Just as we implemented linear regression from scratch, we believe that
softmax regression is similarly fundamental and
you ought to know the gory details of
how to implement it yourself. We limit ourselves to defining the 
softmax-specific aspects and reuse many of the other parts we built 
for regression, such as the training loop.

```{.python .input}
%%tab mxnet
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon
npx.set_np()
```

```{.python .input}
%%tab pytorch
from d2l import torch as d2l
import torch
```

```{.python .input}
%%tab tensorflow
from d2l import tensorflow as d2l
import tensorflow as tf
```

## The Softmax

Let's begin with the most important part, the mapping from scalars to probabilities. 
For a refresher, recall the operation of the sum operator along specific dimensions in a tensor,
as discussed in :numref:`subsec_lin-alg-reduction` and :numref:`subsec_lin-alg-non-reduction`.
[**Given a matrix `X` we can sum over all elements (by default) or only
over elements in the same axis.**]
The `axis` variable lets us compute row and column sums:

```{.python .input}
%%tab all
X = d2l.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
d2l.reduce_sum(X, 0, keepdims=True), d2l.reduce_sum(X, 1, keepdims=True)
```

Computing the softmax requires three steps: 
(i) exponentiation of each term; 
(ii) a sum over each row to compute the normalization constant for each example;
(iii) division of each row by its normalization constant,
ensuring that the result sums to 1.

(**
$$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.$$
**)

The (logarithm of the) denominator is also called the (log) *partition function*. It was 
introduced in [statistical physics](https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics))
to sum over all possible states in a thermodynamic ensemble. The implementation is quite straightforward:

```{.python .input}
%%tab all
def softmax(X):
    X_exp = d2l.exp(X)
    partition = d2l.reduce_sum(X_exp, 1, keepdims=True)
    return X_exp / partition  # The broadcasting mechanism is applied here
```

For any input `X`
[**we turn each element into a non-negative number.
Each row sums up to 1,**]
as is required for a probability. Caution: the code above is *not* robust against very large or very small arguments. While this is sufficient to illustrate what is happening, you should *not* use this code verbatim for any serious purpose. Deep Learning frameworks have such protections built-in and we will be using the built-in softmax going forward.

```{.python .input}
%%tab mxnet
X = d2l.rand(2, 5)
X_prob = softmax(X)
X_prob, d2l.reduce_sum(X_prob, 1)
```

```{.python .input}
%%tab tensorflow, pytorch
X = d2l.rand((2, 5))
X_prob = softmax(X)
X_prob, d2l.reduce_sum(X_prob, 1)
```

## The Model

Now we have everything to implement [**the softmax regression model.**]
As in our linear regression example, each instance will be represented by a 
fixed-length vector. Since the raw data here consists of $28 \times 28$ pixel 
images, [**we flatten each image,
treating them as vectors of length 784.**]
In the future, we will talk about more sophisticated strategies
for exploiting the spatial structure, 
but for now we treat each pixel location as just another feature.

In softmax regression,
we have as many outputs as there are classes.
(**Since our dataset has 10 classes,
our network has an output dimension of 10.**)
Consequently, our weights constitute a $784 \times 10$ matrix
plus a $1 \times 10$ dimensional row vector for the biases. 
As with linear regression, we initialize the weights `W`
with Gaussian noise. The biases to take the initial value 0.

```{.python .input}
%%tab mxnet
class SoftmaxRegressionScratch(d2l.Classification):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = np.random.normal(0, sigma, (num_inputs, num_outputs))
        self.b = np.zeros(num_outputs)
        self.W.attach_grad()
        self.b.attach_grad()

    def collect_params(self):
        return [self.W, self.b]
```

```{.python .input}
%%tab pytorch
class SoftmaxRegressionScratch(d2l.Classification):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),
                              requires_grad=True)
        self.b = torch.zeros(num_outputs, requires_grad=True)

    def parameters(self):
        return [self.W, self.b]
```

```{.python .input}
%%tab tensorflow
class SoftmaxRegressionScratch(d2l.Classification):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.W = tf.random.normal((num_inputs, num_outputs), 0, sigma)
        self.b = tf.zeros(num_outputs)
        self.W = tf.Variable(self.W)
        self.b = tf.Variable(self.b)
```

The code below defines how the input is mapped to the output through the network.
Note that we flatten each $28 \times 28$ pixel image in the batch
into a vector using the `reshape` function before passing the data through our model.

```{.python .input}
%%tab all
@d2l.add_to_class(SoftmaxRegressionScratch)
def forward(self, X):
    return softmax(d2l.matmul(d2l.reshape(X, (-1, self.W.shape[0])), self.W) + self.b)
```

## The Cross-Entropy Loss 

Next we need to implement the cross-entropy loss function 
of :numref:`sec_softmax`. 
This may be the most common loss function
in all of deep learning. At the moment,
classification problems far outnumber regression problems.

Recall that cross-entropy takes the negative log-likelihood
of the predicted probability assigned to the true label.
For efficiency we avoid Python for-loops and use indexing instead. 
In particular, the one-hot encoding in $\mathbf{y}$ 
allows us to select the matching terms in $\hat{\mathbf{y}}$. 

To see this in action we [**create sample data `y_hat`
with 2 examples of predicted probabilities over 3 classes and their corresponding labels `y`.**]
The correct labels are $1$ and $2$ respectively. 
[**Using `y` as the indices of the probabilities in `y_hat`,**]
we can pick out terms efficiently.

```{.python .input}
%%tab mxnet, pytorch
y = d2l.tensor([0, 2])
y_hat = d2l.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]
```

```{.python .input}
%%tab tensorflow
y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y = tf.constant([0, 2])
tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))
```

Now we can (**implement the cross-entropy loss function**) by averaging over the logarithms of the selected probabilities.

```{.python .input}
%%tab mxnet, pytorch
def cross_entropy(y_hat, y):
    return - d2l.reduce_mean(d2l.log(y_hat[range(len(y_hat)), y]))

cross_entropy(y_hat, y)
```

```{.python .input}
%%tab tensorflow
def cross_entropy(y_hat, y):
    return - tf.reduce_mean(tf.math.log(tf.boolean_mask(
        y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))))

cross_entropy(y_hat, y)
```

```{.python .input}
%%tab all
@d2l.add_to_class(SoftmaxRegressionScratch)
def loss(self, y_hat, y):
    return cross_entropy(y_hat, y)
```

## Training

We reuse the `fit` method defined in :numref:`sec_linear_scratch` to [**train the model with 10 epochs.**]
Note that both the number of epochs (`max_epochs`), the minibatch size (`batch_size`)
and learning rate (`lr`) are adjustable hyperparameters.
By changing their values, we may be able
to increase the accuracy of the model.

```{.python .input}
%%tab all
data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)
trainer = d2l.Trainer(max_epochs=10)
trainer.fit(model, data)
```

## Prediction

Now that training is complete,
our model is ready to [**classify some images.**]

```{.python .input}
%%tab all
X, y = next(iter(data.val_dataloader()))
preds = d2l.argmax(model(X), axis=1)
preds.shape
```

We are more interested in the images we label incorrectly. We visualize them by
comparing their actual labels
(first line of text output)
to the predictions from the model
(second line of text output).

```{.python .input}
%%tab all
wrong = d2l.astype(preds, y.dtype) != y
X, y, preds = X[wrong], y[wrong], preds[wrong]
labels = [a+'\n'+b for a, b in zip(
    data.text_labels(y), data.text_labels(preds))]
data.visualize([X, y], labels=labels)
```

## Summary

By now we are starting to get some experience with solving linear regression and classification problems. With it, 
we have reached what would arguably be the state of the art of 1960-1970s of statistical modeling. We are able to solve basic problems efficiently, albeit not quite so well and not quite so elegantly. The next chapter will 
introduce nonlinear models, but before we do so, we need more practice in operating deep learing frameworks efficiently. 

## Exercises

1. In this section, we directly implemented the softmax function based on the mathematical definition of the softmax operation. As discussed in :ref:`sec_softmax` this can cause numerical instabilities. 
    1. Test whether `softmax` still works correctly if an input has a value of $100$?
    1. Test whether `softmax` still works correctly if the largest of all inputs is smaller than $-100$?
    1. Implement a fix by looking at the value relative to the largest entry in the argument.
1. Implement a `cross_entropy` function that follows the definition of the cross-entropy loss function $\sum_i y_i \log \hat{y}_i$. 
    1. Try it out in the code example above.
    1. Why do you think it runs more slowly? 
    1. Should you use it? In which cases would it make sense?
    1. What do you need to be careful of? Hint: consider the domain of the logarithm.
1. Is it always a good idea to return the most likely label? For example, would you do this for medical diagnosis? How would you try to address this?
1. Assume that we want to use softmax regression to predict the next word based on some features. What are some problems that might arise from a large vocabulary?
1. Experiment with the hyperparameters of the code above. In particular:
    1. Plot how the test error changes as you change the learning rate?
    1. Do the test and training error change as you change the minibatch size? How large / small do you need to go before you see an effect?


:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/50)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/51)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/225)
:end_tab:
